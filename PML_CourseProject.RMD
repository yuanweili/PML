---
title: "PMLCourseProject - Human Activity Recognition"
author: "Yuan-Wei Li"
date: "Saturday, December 20, 2014"
output: html_document
---

### Model Building
#### Feature Selection
-Due to the high number of predictors in the training data, it is important to reduce the number of variables to a more manageable numbers. As can be observed, there are many columns in the training data with most of the rows with either NA or blank values. So my first step is to remove those sparse dimensions or features. With that we reduced the number of potential features to 52.

```{r,echo=FALSE, results='hide', warning=FALSE, message=FALSE}

pmlTesting <- read.csv("pml-testing.csv")

pmlTraining <- read.csv("pml-training.csv")


library(caret);
library(lattice);
library(ggplot2);
library(randomForest);
library(rpart);
library(MASS);


vec <- vector()
lapply(names(pmlTraining)[8:159], function(colName) {
        totalRows <-  length(pmlTraining[,colName])       
        useableRows1 <- sum(1 * !is.na(pmlTraining[,colName]))
        useableRows2 <- sum(1 * (nchar(as.character(pmlTraining[,colName])) > 0))
        if((min(c(useableRows1,useableRows2))/totalRows) > 0.9){
                vec <<- c(vec,colName)        
        }
}
)

```

#### The 52 features we will send into PCA:
```{r,echo=FALSE, results='markup', warning=FALSE, message=FALSE}
vec

```

#### PCA
-- The next step is to apply correlation analysis to determine if there are highly correlated features, and ineed so as there are 22 features with > 80% correlation  to at least one other features. So we will proceed to apply PCA to reduce the number of features further. We also tested with nearZeroVar to determine there is no useless features at this point.

```{r,echo=FALSE, results='hide'}
M <- abs(cor(pmlTraining[,vec]))
diag(M) <- 0
which(M > 0.8,arr.ind=T)
#nsv <- nearZeroVar(pmlTraining[,vec],saveMetrics=TRUE)
#nsv
```

#### Data Slicing
--I will use K fold to slice data and use the training data to perform PCA. After perform PCA the number of features is down to 28.
--I will now use the final set of features and compare several models with fold01 of the 10 folds.
```{r,echo=TRUE, results='hide'}
AllTraining <- pmlTraining[,c(vec,"classe")];
remove(pmlTraining);

set.seed(32323)
folds <- createFolds(y=AllTraining$classe,k=10,list=TRUE,returnTrain=FALSE)

training1 <- AllTraining[folds$Fold01,]; 
testing02 <- AllTraining[folds$Fold02,];

preProc <- preProcess(training1[,-53],method="pca")
trainPC <- predict(preProc,training1[,-53])
TrainClasses <- as.factor(training1[,53])

```


### Model Selection
--We will then use the small training set to evaluate several training algorithms: knn (k nearest neighbors), rf (random forest), rpart (tree) , nb (naive Bayes), lda (linear descriminant analysis). Based on the testing results, rf has the lowest out of sample error rate but very slow, knn has the second lowest out of sample error rate but very fast. So I decided to use knn to build my final model.

#### knn (k nearest neighbors)
```{r,echo=TRUE, results='hide'}

Sys.time()

TrainClasses <- as.factor(training1[,53])
knnFit1 <- train(trainPC, TrainClasses,
                 method = "knn",
                 preProcess = c("center", "scale"),
                 tuneLength = 10,
                 trControl = trainControl(method = "repeatedcv"))

testPC <- predict(preProc,testing02[,-53])
Sys.time()

```

-- Performance of knn
```{r}
confusionMatrix(testing02$classe,predict(knnFit1,testPC))
plot(knnFit1, scales = list(x = list(log = 10)))
```

#### The other models (code only, will not execute)

```{r}
#Sys.time()

#rfFit <- train(trainPC,
#               as.factor(TrainClasses),
#               method="rf",               
#               prox=TRUE)

#Sys.time()

#testPC <- predict(preProc,testing02[,-53])
#confusionMatrix(testing02$classe,predict(rfFit,testPC))

#Sys.time()

#rpartFit1 <- train(trainPC, TrainClasses,
#                 method = "rpart",
#                 preProcess = c("center", "scale"),
#                 tuneLength = 10,
#                 trControl = trainControl(method = "repeatedcv", repeats = 3))


#Sys.time()

#confusionMatrix(testing02$classe,predict(rpartFit1,testPC))


#Sys.time()

#ldaFit <- train(trainPC, TrainClasses,preProcess=c("center","scale"),method="lda")

#Sys.time()

#confusionMatrix(testing02$classe,predict(ldaFit,testPC))


#Sys.time()

#nbFit <- train(trainPC, TrainClasses,preProcess=c("center","scale"),method="nb")

#Sys.time()

#confusionMatrix(testing02$classe,predict(nbFit,testPC))
```

### The Final Model
The final model use 70% of the provided training data for training and 30% of provided training data for crosss validation. 

```{r,echo=TRUE, results='hide'}
inTrain <- createDataPartition(y=AllTraining$classe,p=0.7, list=FALSE)
training <- AllTraining[inTrain,]; 
testing <- AllTraining[-inTrain,];
preProc <- preProcess(training[,-53],method="pca")
trainPC <- predict(preProc,training[,-53])
TrainClasses <- as.factor(training[,53])

Sys.time()

knnFit1 <- train(trainPC, TrainClasses,
                 method = "knn",
                 preProcess = c("center", "scale"),
                 tuneLength = 10,
                 trControl = trainControl(method = "repeatedcv"))

Sys.time()

testPC <- predict(preProc,testing[,-53])

```

#### Performance of the final model based on knn.
- The out of sample error rate is 4.18%.
```{r,echo=TRUE, results='markup'}

confusionMatrix(testing$classe,predict(knnFit1,testPC))

```

#### And the prediction for the 20 test cases:
```{r,echo=TRUE, results='markup'}
rTestPC <- predict(preProc,pmlTesting[,vec])
predict(knnFit1,rTestPC)

```
